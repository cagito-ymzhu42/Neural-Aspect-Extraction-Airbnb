{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/sidd/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:174: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/sidd/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:190: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "GPU: ['/job:localhost/replica:0/task:0/device:GPU:0']\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "import tensorflow as tf\n",
    "config = tf.ConfigProto( device_count = {'GPU': 1 , 'CPU': 56} ) \n",
    "sess = tf.Session(config=config) \n",
    "keras.backend.set_session(sess)\n",
    "\n",
    "from keras import backend as K\n",
    "print(\"GPU:\", K.tensorflow_backend._get_available_gpus())\n",
    "\n",
    "# from tensorflow.python.client import device_lib\n",
    "\n",
    "# def get_available_gpus():\n",
    "#     local_device_protos = device_lib.list_local_devices()\n",
    "#     return [x.name for x in local_device_protos if x.device_type == 'GPU']\n",
    "\n",
    "#get_available_gpus()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put these at the top of every notebook, to get automatic reloading and inline plotting\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "\n",
    "# To print all the output in cells (Deactivate if not necesssary. This is to avoid print() statements in cells)\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split, KFold, GridSearchCV\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics import mean_squared_error, r2_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "listing_df = pd.read_csv(\"data/listings.csv\")\n",
    "reviews_df = pd.read_csv(\"data/reviews.csv\")\n",
    "calendar_df = pd.read_csv(\"data/calendar.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>listing_url</th>\n",
       "      <th>scrape_id</th>\n",
       "      <th>last_scraped</th>\n",
       "      <th>name</th>\n",
       "      <th>summary</th>\n",
       "      <th>space</th>\n",
       "      <th>description</th>\n",
       "      <th>experiences_offered</th>\n",
       "      <th>neighborhood_overview</th>\n",
       "      <th>...</th>\n",
       "      <th>review_scores_value</th>\n",
       "      <th>requires_license</th>\n",
       "      <th>license</th>\n",
       "      <th>jurisdiction_names</th>\n",
       "      <th>instant_bookable</th>\n",
       "      <th>cancellation_policy</th>\n",
       "      <th>require_guest_profile_picture</th>\n",
       "      <th>require_guest_phone_verification</th>\n",
       "      <th>calculated_host_listings_count</th>\n",
       "      <th>reviews_per_month</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>241032</td>\n",
       "      <td>https://www.airbnb.com/rooms/241032</td>\n",
       "      <td>20160104002432</td>\n",
       "      <td>2016-01-04</td>\n",
       "      <td>Stylish Queen Anne Apartment</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Make your self at home in this charming one-be...</td>\n",
       "      <td>Make your self at home in this charming one-be...</td>\n",
       "      <td>none</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>10.0</td>\n",
       "      <td>f</td>\n",
       "      <td>NaN</td>\n",
       "      <td>WASHINGTON</td>\n",
       "      <td>f</td>\n",
       "      <td>moderate</td>\n",
       "      <td>f</td>\n",
       "      <td>f</td>\n",
       "      <td>2</td>\n",
       "      <td>4.07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>953595</td>\n",
       "      <td>https://www.airbnb.com/rooms/953595</td>\n",
       "      <td>20160104002432</td>\n",
       "      <td>2016-01-04</td>\n",
       "      <td>Bright &amp; Airy Queen Anne Apartment</td>\n",
       "      <td>Chemically sensitive? We've removed the irrita...</td>\n",
       "      <td>Beautiful, hypoallergenic apartment in an extr...</td>\n",
       "      <td>Chemically sensitive? We've removed the irrita...</td>\n",
       "      <td>none</td>\n",
       "      <td>Queen Anne is a wonderful, truly functional vi...</td>\n",
       "      <td>...</td>\n",
       "      <td>10.0</td>\n",
       "      <td>f</td>\n",
       "      <td>NaN</td>\n",
       "      <td>WASHINGTON</td>\n",
       "      <td>f</td>\n",
       "      <td>strict</td>\n",
       "      <td>t</td>\n",
       "      <td>t</td>\n",
       "      <td>6</td>\n",
       "      <td>1.48</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows × 92 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       id                          listing_url       scrape_id last_scraped  \\\n",
       "0  241032  https://www.airbnb.com/rooms/241032  20160104002432   2016-01-04   \n",
       "1  953595  https://www.airbnb.com/rooms/953595  20160104002432   2016-01-04   \n",
       "\n",
       "                                 name  \\\n",
       "0        Stylish Queen Anne Apartment   \n",
       "1  Bright & Airy Queen Anne Apartment   \n",
       "\n",
       "                                             summary  \\\n",
       "0                                                NaN   \n",
       "1  Chemically sensitive? We've removed the irrita...   \n",
       "\n",
       "                                               space  \\\n",
       "0  Make your self at home in this charming one-be...   \n",
       "1  Beautiful, hypoallergenic apartment in an extr...   \n",
       "\n",
       "                                         description experiences_offered  \\\n",
       "0  Make your self at home in this charming one-be...                none   \n",
       "1  Chemically sensitive? We've removed the irrita...                none   \n",
       "\n",
       "                               neighborhood_overview  ... review_scores_value  \\\n",
       "0                                                NaN  ...                10.0   \n",
       "1  Queen Anne is a wonderful, truly functional vi...  ...                10.0   \n",
       "\n",
       "  requires_license license jurisdiction_names instant_bookable  \\\n",
       "0                f     NaN         WASHINGTON                f   \n",
       "1                f     NaN         WASHINGTON                f   \n",
       "\n",
       "  cancellation_policy  require_guest_profile_picture  \\\n",
       "0            moderate                              f   \n",
       "1              strict                              t   \n",
       "\n",
       "  require_guest_phone_verification calculated_host_listings_count  \\\n",
       "0                                f                              2   \n",
       "1                                t                              6   \n",
       "\n",
       "  reviews_per_month  \n",
       "0              4.07  \n",
       "1              1.48  \n",
       "\n",
       "[2 rows x 92 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>listing_id</th>\n",
       "      <th>id</th>\n",
       "      <th>date</th>\n",
       "      <th>reviewer_id</th>\n",
       "      <th>reviewer_name</th>\n",
       "      <th>comments</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7202016</td>\n",
       "      <td>38917982</td>\n",
       "      <td>2015-07-19</td>\n",
       "      <td>28943674</td>\n",
       "      <td>Bianca</td>\n",
       "      <td>Cute and cozy place. Perfect location to every...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7202016</td>\n",
       "      <td>39087409</td>\n",
       "      <td>2015-07-20</td>\n",
       "      <td>32440555</td>\n",
       "      <td>Frank</td>\n",
       "      <td>Kelly has a great room in a very central locat...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   listing_id        id        date  reviewer_id reviewer_name  \\\n",
       "0     7202016  38917982  2015-07-19     28943674        Bianca   \n",
       "1     7202016  39087409  2015-07-20     32440555         Frank   \n",
       "\n",
       "                                            comments  \n",
       "0  Cute and cozy place. Perfect location to every...  \n",
       "1  Kelly has a great room in a very central locat...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>listing_id</th>\n",
       "      <th>date</th>\n",
       "      <th>available</th>\n",
       "      <th>price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>241032</td>\n",
       "      <td>2016-01-04</td>\n",
       "      <td>t</td>\n",
       "      <td>$85.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>241032</td>\n",
       "      <td>2016-01-05</td>\n",
       "      <td>t</td>\n",
       "      <td>$85.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   listing_id        date available   price\n",
       "0      241032  2016-01-04         t  $85.00\n",
       "1      241032  2016-01-05         t  $85.00"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "listing_df.head(2)\n",
    "\n",
    "reviews_df.head(2)\n",
    "\n",
    "calendar_df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Uncomment and run this only once\n",
    "\n",
    "import spacy\n",
    "from spacy.tokenizer import Tokenizer\n",
    "\n",
    "puncts = [',', '.', '\"', ':', ')', '(', '-', '!', '?', '|', ';', \"'\", '$', '&', '/', '[', ']', '>', '%', '=', '#', '*', '+', '\\\\', '•',  '~', '@', '£', \n",
    " '·', '_', '{', '}', '©', '^', '®', '`',  '<', '→', '°', '€', '™', '›',  '♥', '←', '×', '§', '″', '′', 'Â', '█', '½', 'à', '…', \n",
    " '“', '★', '”', '–', '●', 'â', '►', '−', '¢', '²', '¬', '░', '¶', '↑', '±', '¿', '▾', '═', '¦', '║', '―', '¥', '▓', '—', '‹', '─', \n",
    " '▒', '：', '¼', '⊕', '▼', '▪', '†', '■', '’', '▀', '¨', '▄', '♫', '☆', 'é', '¯', '♦', '¤', '▲', 'è', '¸', '¾', 'Ã', '⋅', '‘', '∞', \n",
    " '∙', '）', '↓', '、', '│', '（', '»', '，', '♪', '╩', '╚', '³', '・', '╦', '╣', '╔', '╗', '▬', '❤', 'ï', 'Ø', '¹', '≤', '‡', '√', ]\n",
    "\n",
    "def clean_text(x):\n",
    "    x = str(x)\n",
    "    for punct in puncts:\n",
    "        x = x.replace(punct, f' {punct} ')\n",
    "    return x\n",
    "\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "tokenizer = Tokenizer(nlp.vocab)\n",
    "\n",
    "def tokenize_docs(df):\n",
    "    texts = df.comments.apply(str).apply(lambda x: clean_text(x)).tolist()\n",
    "    tokenized_docs = []\n",
    "    for i, doc in enumerate(tokenizer.pipe(texts, batch_size=1000)):\n",
    "        tokenized_docs.append([t.text for t in doc if len(t.text.strip())>0])\n",
    "    return tokenized_docs\n",
    "\n",
    "\n",
    "tokenized_reviews = tokenize_docs(reviews_df)\n",
    "reviews_df[\"comments_t\"] = pd.Series(tokenized_reviews).values\n",
    "reviews_df.to_json(\"data/reviews_tokenized.json\")\n",
    "\n",
    "reviews_df = pd.read_json(\"data/reviews_tokenized.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>listing_id</th>\n",
       "      <th>id</th>\n",
       "      <th>date</th>\n",
       "      <th>reviewer_id</th>\n",
       "      <th>reviewer_name</th>\n",
       "      <th>comments</th>\n",
       "      <th>comments_t</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7202016</td>\n",
       "      <td>38917982</td>\n",
       "      <td>2015-07-19</td>\n",
       "      <td>28943674</td>\n",
       "      <td>Bianca</td>\n",
       "      <td>Cute and cozy place. Perfect location to every...</td>\n",
       "      <td>[Cute, and, cozy, place, ., Perfect, location,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7202016</td>\n",
       "      <td>39087409</td>\n",
       "      <td>2015-07-20</td>\n",
       "      <td>32440555</td>\n",
       "      <td>Frank</td>\n",
       "      <td>Kelly has a great room in a very central locat...</td>\n",
       "      <td>[Kelly, has, a, great, room, in, a, very, cent...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   listing_id        id       date  reviewer_id reviewer_name  \\\n",
       "0     7202016  38917982 2015-07-19     28943674        Bianca   \n",
       "1     7202016  39087409 2015-07-20     32440555         Frank   \n",
       "\n",
       "                                            comments  \\\n",
       "0  Cute and cozy place. Perfect location to every...   \n",
       "1  Kelly has a great room in a very central locat...   \n",
       "\n",
       "                                          comments_t  \n",
       "0  [Cute, and, cozy, place, ., Perfect, location,...  \n",
       "1  [Kelly, has, a, great, room, in, a, very, cent...  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviews_df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "import codecs\n",
    "\n",
    "class MySentences(object):\n",
    "    def __init__(self, dframe):\n",
    "        self.dframe = dframe\n",
    "\n",
    "    def __iter__(self):\n",
    "        for row in self.dframe.iterrows():\n",
    "            yield [x.lower() for x in row[1]['comments_t']]\n",
    "\n",
    "\n",
    "def main(df):\n",
    "    sentences = MySentences(df)\n",
    "    model = gensim.models.Word2Vec(sentences, size=100, window=5, min_count=5, workers=4, )\n",
    "    model.save(\"data/w2v_embeddings\")\n",
    "\n",
    "\n",
    "#uncomment this to run word2vec\n",
    "main(reviews_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('bedrooms', 0.8216997385025024),\n",
       " ('bathrooms', 0.7414470911026001),\n",
       " ('spaces', 0.7390960454940796),\n",
       " ('quarters', 0.672817587852478),\n",
       " ('units', 0.6720730066299438),\n",
       " ('bedroom', 0.6579513549804688),\n",
       " ('room', 0.6351318359375),\n",
       " ('accommodations', 0.6175037622451782),\n",
       " ('levels', 0.6049386262893677),\n",
       " ('beds', 0.5957279205322266)]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check the w2v model\n",
    "w2v_model = gensim.models.Word2Vec.load(\"data/w2v_embeddings\")\n",
    "w2v_model.wv.most_similar(\"rooms\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### lets get the data ready for some deep learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import operator\n",
    "import codecs\n",
    "\n",
    "num_regex = re.compile('^[+-]?[0-9]+\\.?[0-9]*$')\n",
    "\n",
    "def is_number(token):\n",
    "    return bool(num_regex.match(token))\n",
    "\n",
    "def create_vocab(dframe, maxlen=0, vocab_size=0):\n",
    "    total_words, unique_words = 0, 0\n",
    "    word_freqs = {}\n",
    "    top = 0\n",
    "    for row in dframe.iterrows():\n",
    "        words = [x.lower() for x in row[1]['comments_t']]\n",
    "        if maxlen > 0 and len(words) > maxlen:\n",
    "            continue\n",
    "\n",
    "        for w in words:\n",
    "            if not is_number(w):\n",
    "                try:\n",
    "                    word_freqs[w] += 1\n",
    "                except KeyError:\n",
    "                    unique_words += 1\n",
    "                    word_freqs[w] = 1\n",
    "                total_words += 1\n",
    "\n",
    "    print ('   %i total words, %i unique words' % (total_words, unique_words))\n",
    "    sorted_word_freqs = sorted(word_freqs.items(), key=operator.itemgetter(1), reverse=True)\n",
    "\n",
    "    vocab = {'<pad>':0, '<unk>':1, '<num>':2}\n",
    "    index = len(vocab)\n",
    "    for word, _ in sorted_word_freqs:\n",
    "        vocab[word] = index\n",
    "        index += 1\n",
    "        if vocab_size > 0 and index > vocab_size + 2:\n",
    "            break\n",
    "    if vocab_size > 0:\n",
    "        print ('  keep the top %i words' % vocab_size)\n",
    "\n",
    "    #Write (vocab, frequence) to a txt file\n",
    "    vocab_file = codecs.open('data/vocab.txt', mode='w', encoding='utf8')\n",
    "    sorted_vocab = sorted(vocab.items(), key=operator.itemgetter(1))\n",
    "    for word, index in sorted_vocab:\n",
    "        if index < 3:\n",
    "            vocab_file.write(word+'\\t'+str(0)+'\\n')\n",
    "            continue\n",
    "        vocab_file.write(word+'\\t'+str(word_freqs[word])+'\\n')\n",
    "    vocab_file.close()\n",
    "\n",
    "    return vocab\n",
    "\n",
    "\n",
    "def read_data(dframe, maxlen=0):\n",
    "    num_hit, unk_hit, total = 0., 0., 0.\n",
    "    maxlen_x = 0\n",
    "    data_x = []\n",
    "    for row in dframe.iterrows():\n",
    "        words = [x.lower() for x in row[1]['comments_t']]\n",
    "        if maxlen > 0 and len(words) > maxlen:\n",
    "            continue\n",
    "\n",
    "        indices = []\n",
    "        for word in words:\n",
    "            if is_number(word):\n",
    "                indices.append(vocab['<num>'])\n",
    "                num_hit += 1\n",
    "            elif word in vocab:\n",
    "                indices.append(vocab[word])\n",
    "            else:\n",
    "                indices.append(vocab['<unk>'])\n",
    "                unk_hit += 1\n",
    "            total += 1\n",
    "\n",
    "        data_x.append(indices)\n",
    "        if maxlen_x < len(indices):\n",
    "            maxlen_x = len(indices)\n",
    "\n",
    "    print('   <num> hit rate: %.2f, <unk> hit rate: %.2f' % (100*num_hit/total, 100*unk_hit/total))\n",
    "    return data_x, maxlen_x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   6745037 total words, 40378 unique words\n",
      "   <num> hit rate: 0.41, <unk> hit rate: 0.00\n"
     ]
    }
   ],
   "source": [
    "vocab = create_vocab(reviews_df)\n",
    "train_data, max_length = read_data(reviews_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training examples:  84849\n",
      "Length of vocab:  40381\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing import sequence\n",
    "\n",
    "train_x = sequence.pad_sequences(train_data, maxlen=max_length)\n",
    "\n",
    "print('Number of training examples: ', len(train_x))\n",
    "print('Length of vocab: ', len(vocab))\n",
    "\n",
    "def sentence_batch_generator(data, batch_size):\n",
    "    n_batch = len(data) / batch_size\n",
    "    batch_count = 0\n",
    "    np.random.shuffle(data)\n",
    "\n",
    "    while True:\n",
    "        if batch_count == n_batch:\n",
    "            np.random.shuffle(data)\n",
    "            batch_count = 0\n",
    "\n",
    "        batch = data[batch_count*batch_size: (batch_count+1)*batch_size]\n",
    "        batch_count += 1\n",
    "        yield batch\n",
    "\n",
    "def negative_batch_generator(data, batch_size, neg_size):\n",
    "    data_len = data.shape[0]\n",
    "    dim = data.shape[1]\n",
    "\n",
    "    while True:\n",
    "        indices = np.random.choice(data_len, batch_size * neg_size)\n",
    "        samples = data[indices].reshape(batch_size, neg_size, dim)\n",
    "        yield samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting some configs for the training run\n",
    "optimizer = keras.optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999, \n",
    "                                  epsilon=1e-08, clipnorm=10, clipvalue=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-02-05 16:34:52,798 INFO   Building model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/sidd/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-02-05 16:34:52,804 WARNING From /home/sidd/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/sidd/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-02-05 16:34:52,809 WARNING From /home/sidd/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/sidd/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-02-05 16:34:52,855 WARNING From /home/sidd/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "/home/sidd/projects/airbnb_seattle_submission/aspectmodel.py:237: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=[<tf.Tenso..., outputs=Tensor(\"ma...)`\n",
      "  model = Model(input=[sentence_input, neg_input], output=loss)\n",
      "2020-02-05 16:34:53,641 INFO Loading embeddings from: data/w2v_embeddings\n",
      "2020-02-05 16:34:53,644 INFO loading Word2Vec object from data/w2v_embeddings\n",
      "2020-02-05 16:34:54,027 INFO loading wv recursively from data/w2v_embeddings.wv.* with mmap=None\n",
      "2020-02-05 16:34:54,031 INFO setting ignored attribute vectors_norm to None\n",
      "2020-02-05 16:34:54,033 INFO loading vocabulary recursively from data/w2v_embeddings.vocabulary.* with mmap=None\n",
      "2020-02-05 16:34:54,034 INFO loading trainables recursively from data/w2v_embeddings.trainables.* with mmap=None\n",
      "2020-02-05 16:34:54,035 INFO setting ignored attribute cum_table to None\n",
      "2020-02-05 16:34:54,037 INFO loaded data/w2v_embeddings\n",
      "2020-02-05 16:34:59,128 INFO   #vectors: 12215, #dimensions: 100\n",
      "2020-02-05 16:34:59,238 INFO Initializing word embedding matrix\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/sidd/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:199: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-02-05 16:34:59,240 WARNING From /home/sidd/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:199: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/sidd/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:206: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-02-05 16:35:00,097 WARNING From /home/sidd/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:206: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
      "\n",
      "2020-02-05 16:35:02,334 INFO 12094/40381 word vectors initialized (hit rate: 29.95%)\n",
      "2020-02-05 16:35:02,461 INFO Initializing aspect embedding matrix as centroid of kmean clusters\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/sidd/anaconda3/lib/python3.7/site-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-02-05 16:35:10,313 WARNING From /home/sidd/anaconda3/lib/python3.7/site-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from aspectmodel import create_model\n",
    "import keras.backend as K\n",
    "import logging\n",
    "\n",
    "logging.basicConfig(\n",
    "                    #filename='out.log',\n",
    "                    level=logging.INFO,\n",
    "                    format='%(asctime)s %(levelname)s %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "logger.info('  Building model')\n",
    "\n",
    "\n",
    "def max_margin_loss(y_true, y_pred):\n",
    "    return K.mean(y_pred)\n",
    "\n",
    "model = create_model(max_length, vocab, \"data/w2v_embeddings\")\n",
    "# freeze the word embedding layer\n",
    "model.get_layer('word_emb').trainable=False\n",
    "model.compile(optimizer=optimizer, loss=max_margin_loss, metrics=[max_margin_loss])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-02-05 16:35:10,418 INFO --------------------------------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9831a25aa49b44ab9e42c67ddbb9f2d0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=500), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/sidd/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-02-05 16:35:10,929 WARNING From /home/sidd/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/sidd/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:986: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-02-05 16:35:12,724 WARNING From /home/sidd/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:986: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/sidd/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:973: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-02-05 16:35:12,891 WARNING From /home/sidd/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:973: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-02-05 16:35:24,458 INFO Epoch 0, train: 13s\n",
      "2020-02-05 16:35:24,461 INFO Total loss: 2.5932, max_margin_loss: 2.5217, ortho_reg: 0.0714\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aspect 0:\n",
      "['beim', 'juste', 'propia', 'geräumig', '있어서', 'ela', 'especialmente', 'geschmackvoll', 'anfitriones', '매우']\n",
      "Aspect 1:\n",
      "['beverages', 'complimentary', 'drinks', 'teas', 'snack', 'beer', 'refreshments', 'snacks', 'fruits', 'milk']\n",
      "Aspect 2:\n",
      "['prompt', 'communicative', 'responsive', 'thorough', 'emails', 'texted', 'responded', 'sent', 'emailed', 'promptly']\n",
      "Aspect 3:\n",
      "['campus', 'stadiums', 'waterfront', 'south', 'northgate', 'greenlake', 'stadium', 'broadway', 'chinatown', 'fremont']\n",
      "Aspect 4:\n",
      "['return', 'come', 'choose', 'be', 'book', 'go', 'enjoy', 'try', 'continue', 'imagine']\n",
      "Aspect 5:\n",
      "['megan', 'kate', 'kim', 'sean', 'shannon', 'terry', 'susanna', 'lynda', 'laura', 'susan']\n",
      "Aspect 6:\n",
      "['modern', 'spacious', 'cozy', 'bright', 'decor', 'interior', 'furniture', 'layout', 'stylish', 'roomy']\n",
      "Aspect 7:\n",
      "['won', 'll', 'wouldn', 'don', 'can', 'd', 'shouldn', 'should', 'doesn', 'couldn']\n",
      "Aspect 8:\n",
      "['usually', 'twice', 'because', 'dark', 'uncomfortable', 'sometimes', 'annoying', 'actually', 'challenge', 'somewhat']\n",
      "Aspect 9:\n",
      "['fantastic', 'fabulous', 'terrific', 'wonderful', 'great', 'phenomenal', 'awesome', 'delightful', 'brilliant', 'superb']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3e98f1d6aaf345e7a1cebabebff756be",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=500), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-02-05 16:35:31,903 INFO Epoch 1, train: 7s\n",
      "2020-02-05 16:35:31,904 INFO Total loss: 1.1815, max_margin_loss: 1.0730, ortho_reg: 0.1085\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Aspect 0:\n",
      "['beim', 'propia', 'juste', 'especialmente', 'ela', '있어서', 'geräumig', 'wunderbar', '매우', 'seiner']\n",
      "Aspect 1:\n",
      "['beverages', 'drinks', 'complimentary', 'snack', 'beer', 'refreshments', 'snacks', 'teas', 'fruits', 'treats']\n",
      "Aspect 2:\n",
      "['prompt', 'communicative', 'responsive', 'thorough', 'emails', 'responded', 'understanding', 'texted', 'promptly', 'timely']\n",
      "Aspect 3:\n",
      "['stadiums', 'campus', 'waterfront', 'south', 'stadium', 'northgate', 'chinatown', 'greenlake', 'broadway', 'buses']\n",
      "Aspect 4:\n",
      "['return', 'come', 'book', 'choose', 'go', 'be', 'continue', 'try', 'enjoy', 'surely']\n",
      "Aspect 5:\n",
      "['megan', 'terry', 'kate', 'patrick', 'sean', 'susanna', 'kim', 'laura', 'erin', 'shannon']\n",
      "Aspect 6:\n",
      "['modern', 'spacious', 'cozy', 'bright', 'decor', 'interior', 'roomy', 'furniture', 'stylish', 'layout']\n",
      "Aspect 7:\n",
      "['won', 'll', 'don', 'd', 'can', 'wouldn', 'should', 'shouldn', 'dont', 'doesn']\n",
      "Aspect 8:\n",
      "['usually', 'because', 'twice', 'dark', 'since', 'actually', 'stuck', 'somewhat', 'until', 'strange']\n",
      "Aspect 9:\n",
      "['fantastic', 'fabulous', 'terrific', 'wonderful', 'great', 'phenomenal', 'brilliant', 'awesome', 'superb', 'amazing']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ffd42cd3ad14102a9ad88e835bcd9cd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=500), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-02-05 16:35:38,603 INFO Epoch 2, train: 6s\n",
      "2020-02-05 16:35:38,604 INFO Total loss: 1.0003, max_margin_loss: 0.8925, ortho_reg: 0.1078\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Aspect 0:\n",
      "['beim', 'propia', 'especialmente', 'ela', '있어서', 'juste', 'wunderbar', 'geräumig', 'seiner', '매우']\n",
      "Aspect 1:\n",
      "['beverages', 'drinks', 'snack', 'beer', 'complimentary', 'snacks', 'treats', 'refreshments', 'teas', 'fruits']\n",
      "Aspect 2:\n",
      "['prompt', 'communicative', 'thorough', 'responsive', 'emails', 'understanding', 'responded', 'timely', 'promptly', 'proactive']\n",
      "Aspect 3:\n",
      "['stadiums', 'campus', 'waterfront', 'stadium', 'south', 'northgate', 'chinatown', 'buses', 'broadway', 'greenlake']\n",
      "Aspect 4:\n",
      "['return', 'come', 'book', 'choose', 'go', 'continue', 'try', 'be', 'surely', 'again']\n",
      "Aspect 5:\n",
      "['megan', 'terry', 'patrick', 'sean', 'kate', 'susanna', 'kim', 'laura', 'paul', 'erin']\n",
      "Aspect 6:\n",
      "['modern', 'spacious', 'cozy', 'bright', 'decor', 'roomy', 'cosy', 'stylish', 'functional', 'furniture']\n",
      "Aspect 7:\n",
      "['won', 'don', 'should', 'd', 'll', 'dont', 'can', 'wouldn', 'shouldn', 'doesn']\n",
      "Aspect 8:\n",
      "['because', 'usually', 'dark', 'twice', 'since', 'until', 'actually', 'past', 'scheduled', 'stuck']\n",
      "Aspect 9:\n",
      "['fantastic', 'fabulous', 'terrific', 'wonderful', 'great', 'phenomenal', 'brilliant', 'awesome', 'superb', 'amazing']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b2cac5af17084c099665e4b7d7679c1a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=500), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Aspect 0:\n",
      "['beim', 'propia', 'especialmente', 'duties', 'ela', '있어서', 'solidly', 'wunderbar', 'seiner', 'bequem']\n",
      "Aspect 1:\n",
      "['beverages', 'drinks', 'snack', 'beer', 'snacks', 'treats', 'complimentary', 'refreshments', 'teas', 'goodies']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-02-05 16:35:45,695 INFO Epoch 3, train: 6s\n",
      "2020-02-05 16:35:45,697 INFO Total loss: 0.9223, max_margin_loss: 0.8152, ortho_reg: 0.1071\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aspect 2:\n",
      "['communicative', 'prompt', 'thorough', 'responsive', 'understanding', 'emails', 'proactive', 'timely', 'responded', 'promptly']\n",
      "Aspect 3:\n",
      "['stadiums', 'campus', 'waterfront', 'stadium', 'south', 'northgate', 'chinatown', 'buses', 'uw', 'broadway']\n",
      "Aspect 4:\n",
      "['return', 'come', 'book', 'choose', 'go', 'continue', 'try', 'surely', 'again', 'visit']\n",
      "Aspect 5:\n",
      "['terry', 'patrick', 'megan', 'sean', 'kate', 'susanna', 'laura', 'kim', 'paul', 'shannon']\n",
      "Aspect 6:\n",
      "['spacious', 'modern', 'roomy', 'cozy', 'functional', 'bright', 'cosy', 'stylish', 'decor', 'tasteful']\n",
      "Aspect 7:\n",
      "['dont', 'should', 'won', 'don', 'd', 'll', 'can', 'might', 'breaker', 'may']\n",
      "Aspect 8:\n",
      "['because', 'usually', 'dark', 'twice', 'until', 'since', 'past', 'scheduled', 'actually', 'stuck']\n",
      "Aspect 9:\n",
      "['fantastic', 'fabulous', 'terrific', 'wonderful', 'phenomenal', 'great', 'brilliant', 'awesome', 'superb', 'amazing']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "950e6f4734b14db6be072ed55cb92611",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=500), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Aspect 0:\n",
      "['beim', 'duties', 'propia', 'rambunctious', 'especialmente', 'solidly', 'ela', 'frühstück', 'bequem', '있어서']\n",
      "Aspect 1:\n",
      "['beverages', 'drinks', 'snack', 'beer', 'snacks', 'treats', 'refreshments', 'complimentary', 'teas', 'goodies']\n",
      "Aspect 2:\n",
      "['communicative', 'prompt', 'thorough', 'responsive', 'understanding', 'proactive', 'emails', 'timely', 'promptly', 'responded']\n",
      "Aspect 3:\n",
      "['stadiums', 'campus', 'waterfront', 'stadium', 'south', 'northgate', 'chinatown', 'buses', 'broadway', 'uw']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-02-05 16:35:52,996 INFO Epoch 4, train: 6s\n",
      "2020-02-05 16:35:52,998 INFO Total loss: 0.8659, max_margin_loss: 0.7595, ortho_reg: 0.1064\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aspect 4:\n",
      "['return', 'come', 'book', 'choose', 'go', 'continue', 'again', 'surely', 'visit', 'try']\n",
      "Aspect 5:\n",
      "['terry', 'patrick', 'megan', 'sean', 'susanna', 'laura', 'kate', 'paul', 'kim', 'shannon']\n",
      "Aspect 6:\n",
      "['roomy', 'spacious', 'functional', 'modern', 'stylish', 'cosy', 'cozy', 'bright', 'tasteful', 'decor']\n",
      "Aspect 7:\n",
      "['dont', 'should', 'breaker', 'why', 'don', 'won', 'd', 'might', 'may', 'fancy']\n",
      "Aspect 8:\n",
      "['dark', 'usually', 'because', 'until', 'scheduled', 'twice', 'past', 'august', 'since', '6pm']\n",
      "Aspect 9:\n",
      "['fabulous', 'fantastic', 'terrific', 'wonderful', 'phenomenal', 'brilliant', 'great', 'loved', 'awesome', 'superb']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "85c2dbc1974949b1b2e24fde5dc96a59",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=500), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Aspect 0:\n",
      "['duties', 'beim', 'rambunctious', 'propia', 'solidly', 'especialmente', 'ela', 'frühstück', 'bequem', 'willkommen']\n",
      "Aspect 1:\n",
      "['beverages', 'drinks', 'snack', 'beer', 'snacks', 'refreshments', 'treats', 'teas', 'complimentary', 'goodies']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-02-05 16:35:59,981 INFO Epoch 5, train: 6s\n",
      "2020-02-05 16:35:59,984 INFO Total loss: 0.8434, max_margin_loss: 0.7376, ortho_reg: 0.1058\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aspect 2:\n",
      "['communicative', 'prompt', 'thorough', 'responsive', 'understanding', 'proactive', 'timely', 'emails', 'promptly', 'responded']\n",
      "Aspect 3:\n",
      "['stadiums', 'campus', 'waterfront', 'stadium', 'northgate', 'south', 'chinatown', 'buses', 'uw', 'broadway']\n",
      "Aspect 4:\n",
      "['return', 'book', 'come', 'go', 'continue', 'choose', 'again', 'surely', 'visit', 'try']\n",
      "Aspect 5:\n",
      "['terry', 'patrick', 'megan', 'sean', 'laura', 'susanna', 'paul', 'kate', 'shannon', 'kim']\n",
      "Aspect 6:\n",
      "['roomy', 'spacious', 'functional', 'surprisingly', 'stylish', 'modern', 'cosy', 'tasteful', 'bright', 'cozy']\n",
      "Aspect 7:\n",
      "['dont', 'breaker', 'why', 'should', 'fancy', 'fair', 'don', 'might', 'may', 'won']\n",
      "Aspect 8:\n",
      "['dark', 'usually', 'until', 'scheduled', 'because', 'past', 'august', 'twice', 'since', '6pm']\n",
      "Aspect 9:\n",
      "['fabulous', 'fantastic', 'terrific', 'wonderful', 'loved', 'brilliant', 'phenomenal', 'great', 'superb', 'awesome']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ec3c8e189921478285d4656129666800",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=500), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-02-05 16:36:07,843 INFO Epoch 6, train: 7s\n",
      "2020-02-05 16:36:07,844 INFO Total loss: 0.8176, max_margin_loss: 0.7125, ortho_reg: 0.1051\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Aspect 0:\n",
      "['duties', 'beim', 'rambunctious', 'solidly', 'asia', 'propia', 'especialmente', 'willkommen', 'frühstück', 'impaired']\n",
      "Aspect 1:\n",
      "['beverages', 'drinks', 'snack', 'snacks', 'beer', 'refreshments', 'treats', 'teas', 'goodies', 'complimentary']\n",
      "Aspect 2:\n",
      "['communicative', 'prompt', 'thorough', 'responsive', 'understanding', 'proactive', 'timely', 'emails', 'promptly', 'attentive']\n",
      "Aspect 3:\n",
      "['stadiums', 'campus', 'waterfront', 'stadium', 'northgate', 'south', 'chinatown', 'buses', 'uw', 'broadway']\n",
      "Aspect 4:\n",
      "['return', 'book', 'come', 'go', 'continue', 'choose', 'again', 'visit', 'surely', 'try']\n",
      "Aspect 5:\n",
      "['terry', 'patrick', 'megan', 'sean', 'laura', 'paul', 'susanna', 'kate', 'steve', 'cindy']\n",
      "Aspect 6:\n",
      "['roomy', 'surprisingly', 'functional', 'spacious', 'attractive', 'stylish', 'cosy', 'tasteful', 'are', 'modern']\n",
      "Aspect 7:\n",
      "['dont', 'breaker', 'why', 'fancy', 'should', 'fair', 'honestly', 'might', 'otherwise', 'nothing']\n",
      "Aspect 8:\n",
      "['dark', 'until', 'usually', 'scheduled', 'august', 'past', 'because', 'twice', '6pm', 'since']\n",
      "Aspect 9:\n",
      "['fabulous', 'fantastic', 'terrific', 'loved', 'wonderful', 'brilliant', 'phenomenal', 'great', 'superb', 'awesome']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca1e0595f4c54eaabc7483c538b9844c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=500), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Aspect 0:\n",
      "['duties', 'rambunctious', 'beim', 'asia', 'solidly', 'propia', 'willkommen', 'impaired', 'especialmente', 'arbor']\n",
      "Aspect 1:\n",
      "['beverages', 'drinks', 'snack', 'snacks', 'beer', 'refreshments', 'teas', 'treats', 'goodies', 'complimentary']\n",
      "Aspect 2:\n",
      "['communicative', 'thorough', 'prompt', 'responsive', 'understanding', 'proactive', 'timely', 'emails', 'promptly', 'informative']\n",
      "Aspect 3:\n",
      "['stadiums', 'campus', 'waterfront', 'stadium', 'northgate', 'south', 'chinatown', 'buses', 'uw', 'broadway']\n",
      "Aspect 4:\n",
      "['return', 'book', 'come', 'go', 'continue', 'again', 'choose', 'visit', 'surely', 'love']\n",
      "Aspect 5:\n",
      "['terry', 'patrick', 'paul', 'laura', 'sean', 'megan', 'steve', 'susanna', 'cindy', 'kate']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-02-05 16:36:15,673 INFO Epoch 7, train: 7s\n",
      "2020-02-05 16:36:15,685 INFO Total loss: 0.8071, max_margin_loss: 0.7028, ortho_reg: 0.1043\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aspect 6:\n",
      "['are', 'surprisingly', 'roomy', 'were', 'functional', 'spacious', 'attractive', 'stylish', 'cosy', 'tasteful']\n",
      "Aspect 7:\n",
      "['breaker', 'why', 'dont', 'fancy', 'honestly', 'fair', 'otherwise', 'should', 'nothing', 'might']\n",
      "Aspect 8:\n",
      "['scheduled', 'until', 'dark', 'usually', 'august', 'past', 'because', 'twice', '6pm', 'plane']\n",
      "Aspect 9:\n",
      "['fabulous', 'fantastic', 'loved', 'terrific', 'wonderful', 'brilliant', 'phenomenal', 'great', 'superb', 'awesome']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e9ea35c825914e16be2b5cb6698795fd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=500), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Aspect 0:\n",
      "['duties', 'rambunctious', 'beim', 'asia', 'solidly', 'pound', 'willkommen', 'tape', 'othello', 'arbor']\n",
      "Aspect 1:\n",
      "['beverages', 'drinks', 'snack', 'snacks', 'teas', 'refreshments', 'beer', 'treats', 'goodies', 'complimentary']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-02-05 16:36:22,740 INFO Epoch 8, train: 6s\n",
      "2020-02-05 16:36:22,745 INFO Total loss: 0.7972, max_margin_loss: 0.6936, ortho_reg: 0.1036\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aspect 2:\n",
      "['thorough', 'communicative', 'prompt', 'responsive', 'understanding', 'proactive', 'timely', 'informative', 'helpful', 'promptly']\n",
      "Aspect 3:\n",
      "['stadiums', 'campus', 'waterfront', 'stadium', 'south', 'northgate', 'chinatown', 'buses', 'uw', 'broadway']\n",
      "Aspect 4:\n",
      "['return', 'book', 'come', 'again', 'go', 'continue', 'choose', 'visit', 'surely', 'love']\n",
      "Aspect 5:\n",
      "['terry', 'patrick', 'laura', 'paul', 'sean', 'steve', 'cindy', 'megan', 'susanna', 'kate']\n",
      "Aspect 6:\n",
      "['are', 'were', 'surprisingly', 'roomy', 'attractive', 'functional', 'spacious', 'tasteful', 'stylish', 'cosy']\n",
      "Aspect 7:\n",
      "['breaker', 'why', 'dont', 'fancy', 'honestly', 'otherwise', 'fair', 'nothing', 'should', 'probably']\n",
      "Aspect 8:\n",
      "['scheduled', 'dark', 'until', 'august', 'usually', 'past', 'twice', '6pm', 'plane', 'because']\n",
      "Aspect 9:\n",
      "['fabulous', 'fantastic', 'loved', 'terrific', 'wonderful', 'brilliant', 'phenomenal', 'great', 'superb', 'awesome']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "63205b0b3e424235b2e7e876da9f983d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=500), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Aspect 0:\n",
      "['duties', 'rambunctious', 'asia', 'beim', 'solidly', 'pound', 'tape', 'othello', 'willkommen', 'graced']\n",
      "Aspect 1:\n",
      "['beverages', 'drinks', 'snack', 'teas', 'snacks', 'refreshments', 'beer', 'goodies', 'treats', 'condiments']\n",
      "Aspect 2:\n",
      "['thorough', 'communicative', 'prompt', 'responsive', 'understanding', 'proactive', 'informative', 'timely', 'helpful', 'promptly']\n",
      "Aspect 3:\n",
      "['stadiums', 'campus', 'waterfront', 'stadium', 'south', 'northgate', 'chinatown', 'buses', 'uw', 'pier']\n",
      "Aspect 4:"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-02-05 16:36:30,690 INFO Epoch 9, train: 7s\n",
      "2020-02-05 16:36:30,691 INFO Total loss: 0.7797, max_margin_loss: 0.6768, ortho_reg: 0.1029\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "['return', 'book', 'come', 'again', 'go', 'continue', 'choose', 'visit', 'surely', 'love']\n",
      "Aspect 5:\n",
      "['terry', 'patrick', 'paul', 'laura', 'cindy', 'sean', 'steve', 'susanna', 'megan', 'kate']\n",
      "Aspect 6:\n",
      "['are', 'were', 'surprisingly', 'attractive', 'roomy', 'spacious', 'functional', 'tasteful', 'stylish', 'cosy']\n",
      "Aspect 7:\n",
      "['breaker', 'why', 'fancy', 'dont', 'honestly', 'otherwise', 'fair', 'nothing', 'probably', 'strong']\n",
      "Aspect 8:\n",
      "['august', 'scheduled', 'until', 'dark', 'usually', 'past', '6pm', 'plane', 'twice', 'flight']\n",
      "Aspect 9:\n",
      "['fabulous', 'fantastic', 'loved', 'terrific', 'wonderful', 'brilliant', 'phenomenal', 'great', 'superb', 'awesome']\n"
     ]
    }
   ],
   "source": [
    "## Training\n",
    "#\n",
    "from keras.models import load_model\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "from time import time\n",
    "\n",
    "logger.info('--------------------------------------------------------------------------------------------------------------------------')\n",
    "\n",
    "vocab_inv = {}\n",
    "for w, ind in vocab.items():\n",
    "    vocab_inv[ind] = w\n",
    "\n",
    "\n",
    "\n",
    "batches_per_epoch = len(train_x) // 32\n",
    "batches_per_epoch = 500\n",
    "\n",
    "min_loss = float('inf')\n",
    "for ii in range(10):\n",
    "    sen_gen = sentence_batch_generator(train_x ,32)\n",
    "    neg_gen = negative_batch_generator(train_x, 32, 5)\n",
    "    t0 = time()\n",
    "    loss, max_margin_loss = 0., 0.\n",
    "\n",
    "    for b in tqdm(range(batches_per_epoch)):\n",
    "        sen_input = next(sen_gen) \n",
    "        neg_input = next(neg_gen)\n",
    "        neg_input = neg_input[:len(sen_input)]\n",
    "        l = len(sen_input) #+ len(neg_input)\n",
    "        batch_loss, batch_max_margin_loss = model.train_on_batch([sen_input, neg_input], np.ones((l, 1)))\n",
    "        #print(batch_loss, batch_max_margin_loss)\n",
    "        loss += batch_loss / batches_per_epoch\n",
    "        max_margin_loss += batch_max_margin_loss / batches_per_epoch\n",
    "\n",
    "    tr_time = time() - t0\n",
    "\n",
    "    if loss< min_loss:\n",
    "        min_loss = loss\n",
    "        word_emb = model.get_layer('word_emb').get_weights()[0]\n",
    "        aspect_emb = model.get_layer('aspect_emb').get_weights()[0]\n",
    "        word_emb = word_emb / np.linalg.norm(word_emb, axis=-1, keepdims=True)\n",
    "        aspect_emb = aspect_emb / np.linalg.norm(aspect_emb, axis=-1, keepdims=True)\n",
    "        aspect_file = codecs.open('output/aspect.log', 'w', 'utf-8')\n",
    "        model.save_weights('output/model_param')\n",
    "\n",
    "        for ind in range(len(aspect_emb)):\n",
    "            desc = aspect_emb[ind]\n",
    "            sims = word_emb.dot(desc.T)\n",
    "            ordered_words = np.argsort(sims)[::-1]\n",
    "            desc_list = [vocab_inv[w] for w in ordered_words[:10]]\n",
    "            print('Aspect %d:' % ind)\n",
    "            print(desc_list)\n",
    "            aspect_file.write('Aspect %d:\\n' % ind)\n",
    "            aspect_file.write(' '.join(desc_list) + '\\n\\n')\n",
    "\n",
    "    logger.info('Epoch %d, train: %is' % (ii, tr_time))\n",
    "    logger.info('Total loss: %.4f, max_margin_loss: %.4f, ortho_reg: %.4f' % (loss, max_margin_loss, loss-max_margin_loss))\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets get top k words per aspect\n",
    "from collections import defaultdict\n",
    "TOP_K = 100\n",
    "aspect2words = defaultdict(list)\n",
    "word_emb = model.get_layer('word_emb').get_weights()[0]\n",
    "aspect_emb = model.get_layer('aspect_emb').get_weights()[0]\n",
    "word_emb = word_emb / np.linalg.norm(word_emb, axis=-1, keepdims=True)\n",
    "aspect_emb = aspect_emb / np.linalg.norm(aspect_emb, axis=-1, keepdims=True)\n",
    "\n",
    "for ind in range(len(aspect_emb)):\n",
    "    desc = aspect_emb[ind]\n",
    "    sims = word_emb.dot(desc.T)\n",
    "    ordered_words = np.argsort(sims)[::-1]\n",
    "    aspect2words[ind] = set([vocab_inv[w] for w in ordered_words[:TOP_K]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2aspects = defaultdict(list)\n",
    "for aspect, words in aspect2words.items():\n",
    "    for word in words:\n",
    "        w2aspects[word].append(aspect)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1773803    124\n",
       "3615614    124\n",
       "2660384    123\n",
       "251922     122\n",
       "817115     121\n",
       "          ... \n",
       "3652434     88\n",
       "1455269     88\n",
       "4118282     88\n",
       "1651324     88\n",
       "3293438     88\n",
       "Name: listing_id, Length: 100, dtype: int64"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# lets go back to reviews and see what people are talking about for a listing. \n",
    "# top reviewed in this dataset\n",
    "reviews_df['listing_id'].value_counts()[100:200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getaspect2reviews(f_rvws, a2w, w2a):\n",
    "    #print(f_rvws)\n",
    "    score = np.zeros((len(f_rvws), len(a2w)))\n",
    "    a2indices = []\n",
    "    for i, review in enumerate(f_rvws):\n",
    "        words = [x.lower() for x in review]\n",
    "        a2word_pos = defaultdict(list)\n",
    "        cur_word_idx = -100\n",
    "        for word_pos, word in enumerate(words):\n",
    "            indices = w2a.get(word, [])\n",
    "            score[i, indices]+=1\n",
    "            for j in indices:\n",
    "#                 if not word_pos-cur_word_idx<=10:\n",
    "                a2word_pos[j].append(word_pos)\n",
    "#                     cur_word_idx = word_pos\n",
    "        a2indices.append(a2word_pos)\n",
    "    return score, a2indices\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "user_aspect = 3\n",
    "number_of_reviews = 5\n",
    "listing_id = 1023693"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reviews on aspect: 3 for listing 1023693\n",
      "Review: 1\n",
      "Ballard was such a treat ! We were looking for something outside downtown Seattle ,\n",
      "Review: 94\n",
      "midnight . We also really liked the detailed rules and suggestions ( for bars , restaurants , etc . ) she left for us . The apartment is quiet and\n",
      "Review: 118\n",
      "is walkable to so many good things : wine shops , restaurants , parks , grocery stores . Unlike Old Ballard , the neighborhood is also easily accessible from elsewhere\n",
      "Review: 91\n",
      "restaurants very close to the apartment , and downtown Ballard has tons of restaurants and cafes ! I loved it here .\n",
      "Review: 10\n",
      "decorated and in a quiet neighborhood within walking distance to Ballard ' s bars , restaurants , etc . ( and a quick drive to downtown Seattle ) . The\n"
     ]
    }
   ],
   "source": [
    "f_rvws = reviews_df[reviews_df.listing_id == listing_id].comments_t\n",
    "scores, a2indices = getaspect2reviews(f_rvws, aspect2words, w2aspects)\n",
    "top_indices = np.argsort(scores[:, user_aspect])[::-1][:number_of_reviews]\n",
    "\n",
    "print(\"Reviews on aspect: {} for listing {}\".format(user_aspect, listing_id))\n",
    "for review_idx in top_indices:\n",
    "    if len(a2indices[review_idx][user_aspect])>0:\n",
    "        word_pos = random.choice(a2indices[review_idx][user_aspect])\n",
    "        review_txt = f_rvws.iloc[review_idx]\n",
    "        print(\"Review:\", review_idx)\n",
    "        print(\" \".join(review_txt[word_pos-15:word_pos] + review_txt[word_pos:word_pos + 15]))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
